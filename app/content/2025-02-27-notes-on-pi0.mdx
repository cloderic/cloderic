---
title:
  'Notes on: "π0: A Vision-Language-Action Flow Model for General Robot Control"'
description:
  'Reading notes on "π0: A Vision-Language-Action Flow Model for General Robot
  Control" by Authors from Physical Intelligence'
date: 2025-02-27
categories:
  - notes
  - highlights
cover: /content/2025-02-27-notes-on-pi0/pi0-a-vision-language-action-flow-model-for-general-robot-control-cover.png
---

## Reference

- **Title**:
- **Authors**:
- **Affiliation**:
  [Physical Intelligence](https://www.physicalintelligence.company)
- **Links**:
  - [Blog Post](https://www.physicalintelligence.company/blog/pi0),
  - [arXiv preprint](https://arxiv.org/abs/2410.24164).

## Summary

Physical Intelligence (stylized as Pi or π - hence the name of the model) is
[Sergey Levine](https://people.eecs.berkeley.edu/~svlevine/)'s robotics startup
co-founded in early 2024 by a team of prominent AI and robotics experts with a
founding team coming from academia and tech industry. In March 2024, Physical
Intelligence emerged from stealth with a
$70 million seed (!) round, by November 2024, the startup had raised an additional $400
million in funding at a valuation of around \$2 billion.

π0 is their flagship model, this paper is the very thorough tech report they
published in november 2024.

- General idea:
  - leverage internet-scale vision-language model + specific robotics dataset to
    build a foundation model for robotics that can 0-shot taks but also be
    finetuned on a wide range of tasks.
  - Ability to control all sorts of robots at up to 50Hz.
  - Target dexterous tasks: e.g. Laundry folding.
  - Task duration > 100s up to x\*10 minutes (eg table bussing)

## Reading notes

![π0 high level architecture](/public/content/2025-02-27-notes-on-pi0/pi0-model-architecture.png)

- Training recipe
  - Pretraining
    - Objective: Train a generalist base model that can follow language commands
      with rudimentary proficiency
    - Dataset: the "pretraining mixture"
      - diversity is important to acquire base skills and ability to recover
      - Their own dataset: 903M timesteps (10'000 hours)
      - Open source datasets: 90M Timesteps):
        [OXE](https://robotics-transformer-x.github.io) (collaborative robotics
        datasets organized by deep mind),
        [BridgeData v2](https://rail-berkeley.github.io/bridgedata/) and
        [Droid](https://droid-dataset.github.io).
  - Post training
    - Objective: help effective task execution
    - For more complex tasks
    - From small amount of data (efficient) to larger datasets
- π0 model architecture
  - One transformer with two sets of weights (2 experts): VLM handling images +
    prompt and action expert for proprioceptive features and action chunks
  - VLM of 3B parameters: PaliGemma Vision Language model (but framework
    compatible with any VML)
    - Image encoders to the same space as language tokens
    - In their experiment, robots have at most 3 cameras (so 3 image slots)
  - Action expert output that use flow matching (300M parameters - same
    architecture than the VLM but less active weights)
    - Usual VLA uses discretized actions as text tokens which limits
      expressivity.
    - Pi0 uses flow matching
      (https://mlg.eng.cam.ac.uk/blog/2024/01/20/flow-matching.html,
      https://en.wikipedia.org/wiki/Flow-based_generative_model)
      - General idea is to train a network to gradually get closer to the
        desired sequence of action (they call it an action chunk)
        - You do that by first training from demonstration to "denoise" the
          perturbed demonstration (the amount of applied noise is followig a
          shifted beta distribution emphasizing lower timesteps ie noisier
          actions)
        - At inference you start from gaussian noise and you iteratively denoise
          it until you get to the sequence of action you want (they do it in 10
          steps)
  - Inference time for 3 camera image is 73ms on a GeForce RTX generating 50
    actions chunk (so 1 second for 50Hz robots, or 2.5 seconds for 20Hz robots)
    in practice they simply execute the first actions of the chunk until it's
    time for a new inference (depends on the robots in their experiments from
    one decision every 0.5 seconds to one every 0.8 seconds).
  - Token sequence
    - Block 1
      - Sequence of images
      - Language prompt
      - Those as the VLM input, they don't attend to the later blocks
    - Block 2: Proprioceptive state encoded into to a token
      - It doesn't attend to later block making it fixed during flow matching
        (and therefore cacheable)
      - Dimensionality matches the largest robot in the dataset: 18 (6dof arms,
        2 grippers, mobile base, vertical actuated torso). Robots with lower
        dimensional configuration are zero padded.
    - Block 3: Noisy action chunk (starting with gaussian noise)
      - Mapped to the transformer's embedding using a MLP
      - That's the part that is "denoised" / integrated using flow matching at
        inference
      - The action chunk is the output of the model, it's the sequence of
        actions that the robot will execute.
- Evaluations:

  - How well π0 perform after the pretraining
    - compared to other foundation robot model OpenVLA and Octo on seen tasks
    - 4 tasks seen in the training set (folding shirt, bussing table easy and
      hard, bagging groceries, removing toast from toaster)
    - Open VLA is also a ~7B VLA model, originally trained on the OXE dataset,
      it was retrained on their dataset for these experiments but it's quite
      challening from π0 as it has no way of dealing with high frequency
      decision making. Only trained for 160k steps. Also provided a UR5e only
      trained version w/o cross-embodiment.
    - Octo, not a VLA and smaller (93M parameters) uses a diffusion process to
      generate actions. Also retrained on the same data.
    - Evaluation metric is an average normalized score over 10 episodes, 1.0
      means fully completed the task. < 1.0 score mean that only part of the
      task was done (e.g. removing 2 toasts out of the 4 gives 0.5). For some
      tasks there's a limited time to achieve it.
      - Note -> we can see that we are early on those benchmark because it's
        only about doing the task not about how fast, how efficient or how
        aligned it is.
    - π0 is way better than the other models even at training parity (ie same
      number of training steps).
  - How well π0 follow language commands

    - compared to π0-small that doesn't have the VLM part but follow the same
      training procedure and similar architecture
    - π0 small doesn't use any pretrained VLM, it's significantly smaller (470M)
      - The language instructions are encoded using distilbert embedding
      - Images are encoded using a pretrained vit-resnet hybrid (one per image
        input, not sharing weights)
      - Both outpus, as well as the proprioceptive data are concatenated and fed
        into a transformer backbone (that has no pretraining)
      - Action expert uses a DiT (diffusion transfomer)
      - Note: admittedly, I'm no expert but what I understand is that due to the
        different architecture, the action expert can't attend to the raw
        language and vision tokens (they don't exist) but only to the output of
        the ViTs and the embedding, which means it's more restricted in its
        access to the raw information. Dear reader, if I'm wrong let me know!
      - Note: This is relatively similar architecture to what we built at AIR in
        the GLIDE-RL paper: https://arxiv.org/abs/2401.02991 - Except we trained
        our architecture in simulation with a RL approach and we don't have the
        vision part.
    - 3 setups:
      - _flat_ -> Only the high level task is provided (e.g. "set the table")
      - _human_ -> Language instructions are provided for segments of ~2
        seconds: which object to pick up, where to put it (each tasks has a lot
        of segments: bussing, setting a table, bagging groceries), it was
        annotated by human experts.
        - Note: I'm not how _human_ it is, it seems like no human would like to
          micromanage a robot like that
      - _hl_ -> use a high level VLM to generate intermediate actions
        - Note: They don't evaluate π0-small with that modality, I'm not sure
          why that would be interesting to understand if a high level VLM with
          low frequency updates can compensate for a simpler low level decision
          making model (intuitively it should).
    - π0 is better at following instructions.
    - π0 gets better with low level instructions.

  - How well π0 perform on dexterous tasks with specific fine tuning dataset
    - π0 is evaluated in 3 modes: pretraining + posttraining with more or less
      data (1h to 10h), training from scratch with the task data.
    - Pool of new tasks some of them with partial representation in the initial
      training set (easy tasks) some with none (hard tasks).
    - This time they used public checkpoints for OpenVLA and Octo. They then
      fine tuned the models on the tasks.
    - Also adding ACT and Diffusion policy trained on just the tasks.
    - Result -> π0 is better, the pretraining does not seems to be that
      important
  - Can π0 be adapted to complex multi-stage tasks (5 to 20 minutes to complete)
    - Using a combination of fine tuning + high level instructions, trying to
      tackle more complex tasks requiring More generalization, unseen objects
    - Compare π0 out of the box, π0 fine tuned on the task, π0 train from
      scratch on the task
      - Generally fine tuning is the best, from scratch has very variable
        performances.

## Conclusion

## Final Thoughts

- **Strengths**:
  - LLM approach to robotics -> pretraining on "internet"-scale data, post
    training on specific tasks
  - Very clear that this is a stepping stone, not a final
- **Limitations**:
  - Prorietary training dataset, very large, no real recipe on how to gather it
    (they talk about that in their conclusion)
  - Fine tuning could use some force of performance optimization
- **Odds and ends**:
