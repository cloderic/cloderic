---
title:
  'Notes on: "π0: A Vision-Language-Action Flow Model for General Robot Control"'
description:
  'Reading notes on "π0: A Vision-Language-Action Flow Model for General Robot
  Control" by Authors from Physical Intelligence'
date: 2025-02-XX
categories:
  - notes
  - highlights
cover: /content/YYYY-MM-DD-notes-on-paper/paper-cover.png
---

## Reference

- **Title**:
- **Authors**:
- **Affiliation**:
  [Physical Intelligence](https://www.physicalintelligence.company)
- **Links**:
  - [Blog Post](https://www.physicalintelligence.company/blog/pi0),
  - [arXiv preprint](https://arxiv.org/abs/2410.24164).

## Summary

- General idea:
  - leverage internet-scale vision-language model + specific robotics dataset to
    build a foundation model for robotics that can 0-shot but also be finetuned
    on a wide range of tasks.
  - Ability to control all sorts of robots at up to 50Hz
  - Target dexterous tasks: e.g. Laundry folding.
  - Task duration > 100s up to x\*10 minutes (eg table bussing)
- Training recipe
  - Pretraining
    - Objective: Train a generalist base model that can follow language commands
      with rudimentary proficiency
    - Dataset
      - diversity is important to acquire base skills and ability to recover
      - from their own dataset
      - OXE (collaborative robotics datasets organized by deep mind)
  - Post training
    - Objective: help effective task execution
    - For more complex tasks
    - From small amount of data (efficient) to larger datasets
- Model architectures
  - One transformer with two sets of weights: VLM handling images + prompt and
    action expert for proprioceptive features and action chunks
    - However the action expert
  - VLM of 3B parameters: PaliGemma Vision Language model (but framework
    compatible with any VML)
    - Image encoders to the same space as language tokens
    - In their experiment, robots have at most 3 cameras (so 3 image slots)
  - Action expert output that use flow matching (300M parameters)
    - 3 set of experts for 3 different action spaces in their experiments
    - Usual VLA uses discretized actions as text tokens.
    - Pi0 uses flow matching
      (https://mlg.eng.cam.ac.uk/blog/2024/01/20/flow-matching.html,
      https://en.wikipedia.org/wiki/Flow-based_generative_model)
      - General idea is to train a network to gradually get closer to the
        desired sequence of action (they call it an action chunk)
        - You do that by first training from demonstration to "denoise" the
          perturbed demonstration (the amount of applied noise is followig a
          shifted beta distribution emphasizing lower timesteps ie noisier
          actions)
        - At inference you start from gaussian noise and you iteratively denoise
          it until you get to the sequence of action you want (they do it in 10
          steps)
    - Based on transfusion
  - Inference time for 3 camera image is 73ms on a GeForce RTX generating 50
    action chunk (so 1 second at 50Hz) in practice they simply run the first
    ones and rerun faster than that.
  - Input tokens
    - Block 1
      - Sequence of images
      - Language prompt
      - Those as the VLM input, they don't atten to the later blocks
    - Block 2: Proprioceptive state (singular) token mapped to the transformer's
      embedding using a linear projection (?)
      - It doesn't attend to later block making it fixed during flow matching
        (and therefore cacheable)
      - Dimensionality matches the largest robot in the dataset: 18 (6dof arms,
        2 grippers, mobile base, vertical actuated torso). Robots with lower
        dimensional configuration are zero padded.
    - BLock 3: Noisy action chunk (starting with gaussian noise)
      - Mapped to the transformer's embedding using a MLP
  - Output
    - Action sequence
  - Also ran experiment with a higher level LLM that decomposes the task at
    hand.
- Evaluation:
  - Compared against OpenVLA and Octo
  - π0-small doens't have the VML part

## Reading notes

## Final Thoughts

- **Strengths**:
- **Limitations**:
- **Odds and ends**:
